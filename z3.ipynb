{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e762996c-6980-407b-bf30-534704ad69b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# z3: Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39437265-2445-4f32-8581-47b54a3d3a82",
   "metadata": {},
   "source": [
    "## Data Engineering Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3549b-0ebc-409d-9ce2-aa85c0189687",
   "metadata": {},
   "source": [
    "### First run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904ae87d-8b6b-4425-bc3e-873c8637eb71",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from base.z3_base import z3Base\n",
    "from z3_nodes.z3_inventory import z3Inventory\n",
    "from z3_nodes.z3_sellout import z3Sellout\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5227e8-755a-4cfc-8e45-6fe2a24b93ce",
   "metadata": {},
   "source": [
    "### Then run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d905d06f-1ee2-46f3-9f2a-70562dbe2d3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<z3_nodes.z3_sellout.z3Sellout at 0x7fa4b0eeebe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3Sellout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02797b7-f759-4872-8084-aef766c31223",
   "metadata": {},
   "source": [
    "### Then run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc422df-2dab-4ac8-9f66-a965c02cdfc8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<z3_nodes.z3_inventory.z3Inventory at 0x7fa4b0eeeca0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3Inventory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdad8cc-bfc6-4578-be34-dd5d41f7ac7d",
   "metadata": {},
   "source": [
    "### Run the cell below only if you want to delete the tables in z3_results database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985d487e-5a36-4547-93ac-8572f9311ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "z3Base.drop_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94e170-7140-4abb-812a-5664db0895ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c51c2d1-208c-4de4-8de4-3b0cc8718638",
   "metadata": {},
   "source": [
    "## I have done all my project on python files, from here I will use code snippets to explain each step on the process, the cells are not intended to run exactly as is intended in the \"run the program\" part above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37beda2d-c8e0-4ff4-a095-b415620c53e6",
   "metadata": {},
   "source": [
    "## Step 1: Scope the project and gather the data\n",
    "I will use the data from our clients databases,  I will anonymize an example of the raw data extracted from the databases, the data sources which are each database are more than 2 as it's expected in the project rubric. As it's defined in the anonymized datasets, the raw data extracted from the database surpasses 5 million rows on each report type (sellout, inventory). From those rows it summarizes the indicators pos qty, pos sales (sellout) and curr on hand qty (inventory) and groups it into the report daily (the date to which the data corresponds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7adf569f-0de4-4e24-b604-94820f339098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily</th>\n",
       "      <th>scrapper_pos_qty</th>\n",
       "      <th>scrapper_pos_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>111.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022/04/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        daily  scrapper_pos_qty  scrapper_pos_sales\n",
       "0  2022/04/11               1.0               45.74\n",
       "1  2022/04/11               1.0               46.55\n",
       "2  2022/04/11               1.0               50.00\n",
       "3  2022/04/11               1.0               91.20\n",
       "4  2022/04/11               1.0               97.41\n",
       "5  2022/04/11               1.0              111.02\n",
       "6  2022/04/11               1.0              103.36\n",
       "7  2022/04/11               1.0               95.37\n",
       "8  2022/04/11               1.0               97.41\n",
       "9  2022/04/11               1.0              103.36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
    "\n",
    "df = pd.read_csv('raw_data_SELLOUT.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211d091-461d-4c5a-9549-0b477346d323",
   "metadata": {},
   "source": [
    "## Step 2: Explore and asses the data\n",
    "As I said in the project scope the two most important problems for us are when indicators have an extremely high value (duplicated) or extremely low values, I will solve this by taking the outlier limits that are calculated by substracting inter quartile range * 1.5 to quartile 1 or adding it to quartile 3, but instead I will use inter quartil range * 3, because I dont need to find data variability outliers instead only extreme ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d446ff-7589-4744-aeb6-f0a9af730455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
    "\n",
    "df = pd.read_csv('raw_data_SELLOUT.csv')\n",
    "df = df.groupby('daily').aggregate('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc900cf-6133-416e-84a2-a3e3e612125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scrapper_pos_qty</th>\n",
       "      <th>scrapper_pos_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31.00000</td>\n",
       "      <td>31.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1455139.27057</td>\n",
       "      <td>34048785.33055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>310791.69812</td>\n",
       "      <td>6636149.44017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>828393.10000</td>\n",
       "      <td>16811180.81787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1312710.50085</td>\n",
       "      <td>32506898.93931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1528647.35510</td>\n",
       "      <td>35323766.01079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1596825.05735</td>\n",
       "      <td>37921888.60274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2058449.09200</td>\n",
       "      <td>46030551.08599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       scrapper_pos_qty  scrapper_pos_sales\n",
       "count          31.00000            31.00000\n",
       "mean      1455139.27057      34048785.33055\n",
       "std        310791.69812       6636149.44017\n",
       "min        828393.10000      16811180.81787\n",
       "25%       1312710.50085      32506898.93931\n",
       "50%       1528647.35510      35323766.01079\n",
       "75%       1596825.05735      37921888.60274\n",
       "max       2058449.09200      46030551.08599"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb0e7d-288e-421a-9523-3e420f8aace2",
   "metadata": {},
   "source": [
    "## Step 3: Define the data model\n",
    "#### Conceptual model\n",
    "My data model will be a star schema, since we need to analyze specifically numeric indicators, that will tell us what's wrong with the data gathered by the robots, it will be easier for joins and centered on the quantitative indicators.\n",
    "\n",
    "\n",
    "### Database tables\n",
    "#### Fact Table\n",
    "\n",
    "* **indicators** - quantitative values that show the performance of a product.\n",
    "    * client_id, report_id, provider_id, execution_id, daily_id, scrapper_pos_qty, scrapper_pos_sales, scrapper_curr_on_hand_qty, scrapper_rows.\n",
    "\n",
    "#### Dimension Tables\n",
    "\n",
    "* **clients** - Our clients.\n",
    "    * client_id, client\n",
    "* **daily** - The date to which the data belongs.\n",
    "    * daily_id, daily, daily_year, daily_month, daily_day.\n",
    "* **reports** - The report type of the data uploaded.\n",
    "    * report_id, report_type.\n",
    "* **providers** - The store type/brand where the products are being sold. \n",
    "    * provider_id, provider.\n",
    "* **executions** - The identifier of the data that is loaded into the z3_results database once you run any of the nodes (sellout or inventory).\n",
    "    * execution_id, execution_date, execution_year, execution_month, execution_day, execution_hour, execution_minute.\n",
    "\n",
    "#### Mapping out data pipelines\n",
    "* 1. Summarize the data in the database of each client, grouping the results by daily, and saving the columns provider, client, pos qty, pos sales and curr on hand qty columns.\n",
    "* 2. Process the past into a dataframe and apply statistical rules described on step 2 to filter the values we want.\n",
    "* 3. Take the result into a master of all clients and providers, and posible days with extreme values, create the id of each of the dimension tables.\n",
    "* 4. Slice the master dataframe into the star schema tables.\n",
    "* 5. Perform the loading queries for each table.\n",
    "* 6. Run the tests to confirm the data loaded has the same values as the data extracted and processed, I do this with 5 tests, each one correlates with the 3 principal indicators, also the scrapper rows that correspond to the original report uploaded into the clients database, and the total dailys in the results of step 2 must be present in the dailys uploaded that correlate with the execution id.\n",
    "* 7. Apply the zzz, and sleep better, you will be aware if the websites are returning useless data, and the clients will be happy for it.\n",
    "* 8. If needed drop the tables by running the drop_tables method on jupyter lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd38ebae-3c08-4049-b626-7f83a2b6c474",
   "metadata": {},
   "source": [
    "## Step 4: Run pipelines to model the data\n",
    "The pipelines are defined in the **base/z3_base.py** file, the most important methods that let us execute the pipelines are **extract_and_transform_each_provider_and_client()** it's too clear what it does haha! **load()** that takes the results into star schema and uploads into the z3_results database, **data_quality_checks()** that performs tests and **drop_tables()** that gets rid of the tables if needed. \n",
    "\n",
    "The data dictionary is in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f32e7a-8cf6-4e8b-90a1-f78366959b01",
   "metadata": {},
   "source": [
    "### The next cell is the z3_base.py file that is the heart of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b15fd20-970e-43af-90b6-5a6b730b6106",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime as dt\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import pytest\n",
    "import pytz\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from pandas import DataFrame\n",
    "\n",
    "from base.constants import columns_quality_checks\n",
    "from base.z3_interface import z3Interface\n",
    "from engineering.engineering import (change_column_datatype,\n",
    "                                     create_date_yyyy_mm_dd,\n",
    "                                     create_date_yyyy_mm_dd_hh_mins)\n",
    "from extract_and_quality.extract_and_quality_queries import quality_checks\n",
    "from load.load_queries import (create_table_queries, drop_table_queries,\n",
    "                               insert_table_queries_dict)\n",
    "\n",
    "\n",
    "class z3Base(z3Interface):\n",
    "    \"\"\"Class that defines and directs the execution of the etl process.\"\"\"\n",
    "    DAILY_FORMAT: str = \"%Y/%m/%d\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Defines the variables that will be used on the etl process.\"\"\"\n",
    "        self.z3_tables_dictionary: Dict[str, str] = {}\n",
    "        load_dotenv(find_dotenv())\n",
    "\n",
    "        self.client_ids = ast.literal_eval(os.getenv(\"CLIENT_IDS\"))\n",
    "        self.provider_ids = ast.literal_eval(os.getenv(\"PROVIDER_IDS\"))\n",
    "        self.report_ids = ast.literal_eval(os.getenv(\"REPORT_IDS\"))\n",
    "\n",
    "        self.z3_indicators_master: DataFrame = pd.DataFrame()\n",
    "        self.z3_dates_table: DataFrame = pd.DataFrame()\n",
    "        self.z3_reports_table: DataFrame = pd.DataFrame()\n",
    "        self.z3_clients_table: DataFrame = pd.DataFrame()\n",
    "        self.z3_providers_table: DataFrame = pd.DataFrame()\n",
    "        self.z3_indicators_table: DataFrame = pd.DataFrame()\n",
    "        self.z3_executions_table: DataFrame = pd.DataFrame()\n",
    "\n",
    "        self.z3_raw_data: DataFrame = pd.DataFrame()\n",
    "\n",
    "        self.z3_results_password_db: str = os.getenv(\"RESULTS_PASSWORD\")\n",
    "        self.z3_results_user_db: str = os.getenv(\"RESULTS_USER_DB\")\n",
    "        self.z3_results_host_db: str = os.getenv(\"RESULTS_HOST_DB\")\n",
    "\n",
    "    def extract_and_transform_each_provider_and_client(self):\n",
    "        \"\"\"Performs the extraction that queries each database for each client\n",
    "        then process the result of the query to filter the data and get only\n",
    "        the extreme values that could be an error from the site.\"\"\"\n",
    "        for self.client in self.clients:\n",
    "            provider_and_config_report_id: Dict[str, Dict] = ast.literal_eval(\n",
    "                os.getenv(self.client.upper().strip()))\n",
    "            provider_and_config_report_id: Dict[str, str] = provider_and_config_report_id.get(\n",
    "                self.report_type)\n",
    "            providers: List[str] = list(provider_and_config_report_id.keys())\n",
    "\n",
    "            self.password_db: str = os.getenv(\"PASSWORD\")\n",
    "            self.user_db: str = os.getenv(\"USER_DB\")\n",
    "            self.host_db: str = os.getenv(\"HOST_DB\") if self.client.upper() not in os.getenv(\n",
    "                \"CLIENTS_DB2\") else os.getenv(\n",
    "                \"HOST_DB_2\")\n",
    "\n",
    "            for self.provider in providers:\n",
    "                self.config_report: int = provider_and_config_report_id.get(\n",
    "                    self.provider)\n",
    "                z3_df, empty_df = self._extract()\n",
    "                if not empty_df:\n",
    "                    z3_indicators, z3_indicators_empty = self._transform(\n",
    "                        z3_df=z3_df)\n",
    "                    if not z3_indicators_empty:\n",
    "                        self.z3_indicators_master = pd.concat(\n",
    "                            [z3_indicators, self.z3_indicators_master])\n",
    "\n",
    "        self.z3_raw_data.to_csv(f'raw_data_{self.report_type}.csv', index=False)\n",
    "        tz = pytz.timezone('America/Mexico_City')\n",
    "        today_mx: dt.date = dt.datetime.now(tz=tz).today()\n",
    "        today_str: str = today_mx.strftime('%Y/%m/%d-%H:%M')\n",
    "        self.z3_indicators_master['execution_date'] = today_str\n",
    "\n",
    "    def _get_date_range(self) -> Tuple[str, str]:\n",
    "        \"\"\"Defines tha date range that will be queried amongst the databases.\n",
    "\n",
    "        @return thirty_days_ago: the date that corresponds thirty days ago before yesterday.\n",
    "        @return yesterday: the date that corresponds to the date before today.\n",
    "        \"\"\"\n",
    "        tz = pytz.timezone('America/Mexico_City')\n",
    "        today_mx: dt.date = dt.datetime.now(tz=tz).today()\n",
    "        yesterday: dt.date = (today_mx - dt.timedelta(days=1))\n",
    "        thirty_days_ago: dt.date = (yesterday - dt.timedelta(days=30))\n",
    "        yesterday: str = yesterday.strftime(self.DAILY_FORMAT)\n",
    "        thirty_days_ago: str = thirty_days_ago.strftime(self.DAILY_FORMAT)\n",
    "\n",
    "        return thirty_days_ago, yesterday\n",
    "\n",
    "    def _extract(self) -> Tuple[DataFrame, bool]:\n",
    "        \"\"\"Performs the extraction query and inserts the client, and provider\n",
    "        of each request.\n",
    "\n",
    "        @return df: the dataframe that is the result of the query.\n",
    "        @return empty_df: a boolean that tells you if the dataframe is empty or not.\n",
    "        \"\"\"\n",
    "        df: DataFrame = self._perform_extract_query()\n",
    "        self.z3_raw_data = pd.concat([df, self.z3_raw_data])\n",
    "        df['scrapper_rows'] = 1\n",
    "        if self.report_type == 'INVENTORY':\n",
    "            change_column_datatype(df, 'scrapper_curr_on_hand_qty', 'float')\n",
    "        else:\n",
    "            change_column_datatype(df, 'scrapper_pos_sales', 'float')\n",
    "            change_column_datatype(df, 'scrapper_pos_qty', 'float')\n",
    "\n",
    "        df: DataFrame = df.groupby('daily').aggregate('sum')\n",
    "        df: DataFrame = pd.DataFrame(df.reset_index())\n",
    "        df['client'] = self.client\n",
    "        df['provider'] = self.provider\n",
    "        empty_df: int = df.shape[0]\n",
    "        empty_df: bool = empty_df == 0\n",
    "        return df, empty_df\n",
    "\n",
    "    def _perform_extract_query(self) -> DataFrame:\n",
    "        \"\"\"Connects to the database selected for each client and does the\n",
    "        query, also gets the result into a dataframe format.\n",
    "\n",
    "        @return df: the data frame that is the result of the query.\n",
    "        \"\"\"\n",
    "        thirty_days_ago, yesterday = self._get_date_range()\n",
    "        db_name: str = f\"scrappers_{self.client.lower()}\"\n",
    "        connection: str = f\"host={self.host_db} dbname={db_name} user={self.user_db} password={self.password_db}\"\n",
    "        df: DataFrame = pd.DataFrame()\n",
    "        conn = psycopg2.connect(connection)\n",
    "        try:\n",
    "            conn.set_session(autocommit=True, readonly=True)\n",
    "            cur = conn.cursor()\n",
    "            query_tuple: Tuple = (self.config_report,\n",
    "                                  thirty_days_ago,\n",
    "                                  yesterday,\n",
    "                                  thirty_days_ago,\n",
    "                                  yesterday)\n",
    "\n",
    "            cur.execute(self.query_db, query_tuple)\n",
    "            df: DataFrame = DataFrame(\n",
    "                cur.fetchall(),\n",
    "                columns=self.columns,\n",
    "            )\n",
    "        except psycopg2.Error as e:\n",
    "            logging.info(e)\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _transform(self, z3_df: DataFrame) -> Tuple[DataFrame, bool]:\n",
    "        \"\"\"Once we have the raw data this method will filter to look for really\n",
    "        extreme values.\n",
    "\n",
    "        @param z3_df: the raw data from the database.\n",
    "        @return z3_unified: since the data frame is filtered by extreme results for 3 different columns\n",
    "            this dataframe corresponds to each result on the columns altogether in one dataframe.\n",
    "        \"\"\"\n",
    "        indicator_2_emptiness: bool = True\n",
    "        z3_indicator_1, indicator_1_emptiness = self._transform_quantitative_indicators(\n",
    "            z3_df,\n",
    "            self.key_performance_indicator_1\n",
    "        )\n",
    "\n",
    "        if self.key_performance_indicator_2:\n",
    "            z3_indicator_2, indicator_2_emptiness = self._transform_quantitative_indicators(\n",
    "                z3_df,\n",
    "                self.key_performance_indicator_2\n",
    "            )\n",
    "\n",
    "        z3_rows, rows_emptiness = self._transform_quantitative_indicators(\n",
    "            z3_df,\n",
    "            'scrapper_rows'\n",
    "        )\n",
    "\n",
    "        z3_unified: DataFrame = pd.DataFrame()\n",
    "        if not indicator_1_emptiness:\n",
    "            z3_unified: DataFrame = pd.concat([z3_indicator_1, z3_unified])\n",
    "\n",
    "        if not indicator_2_emptiness and self.key_performance_indicator_2:\n",
    "            z3_unified: DataFrame = pd.concat([z3_indicator_2, z3_unified])\n",
    "\n",
    "        if not rows_emptiness:\n",
    "            z3_unified: DataFrame = pd.concat([z3_rows, z3_unified])\n",
    "\n",
    "        z3_unified: DataFrame = z3_unified.drop_duplicates(keep='first')\n",
    "\n",
    "        z3_unified_empty: bool = bool(\n",
    "            indicator_2_emptiness and indicator_1_emptiness)\n",
    "\n",
    "        return z3_unified, z3_unified_empty\n",
    "\n",
    "    def _transform_quantitative_indicators(self, df: DataFrame, column: str) -> Tuple[DataFrame, bool]:\n",
    "        \"\"\"\n",
    "        Receives the raw dataframe and filters the column for its extreme values, using iqr * 3 to look\n",
    "        for extreme outliers.\n",
    "        @param df: the raw dataframe.\n",
    "        @param column: the column to be filtered quantitatively\n",
    "        @return df_fails: the result of the filter that are the possible fails from the website scrapped.\n",
    "        @return empty_df: a boolean that shows if there wasnt any extreme results.\n",
    "        \"\"\"\n",
    "        first_q: float = np.percentile(df[column], 25)\n",
    "        third_q: float = np.percentile(df[column], 75)\n",
    "\n",
    "        iqr: float = third_q - first_q\n",
    "        lower_limit: float = first_q - (iqr * 3)\n",
    "        lower_limit: float = lower_limit if lower_limit > 0 else 1\n",
    "        upper_limit: float = third_q + (iqr * 3)\n",
    "\n",
    "        df_lower: DataFrame = df[df[column] < lower_limit]\n",
    "        df_upper: DataFrame = df[df[column] > upper_limit]\n",
    "\n",
    "        df_fails: DataFrame = pd.DataFrame()\n",
    "        df_fails: DataFrame = pd.concat([df_lower, df_fails])\n",
    "        df_fails: DataFrame = pd.concat([df_upper, df_fails])\n",
    "\n",
    "        empty_df: int = df_fails.shape[0]\n",
    "        empty_df: bool = empty_df == 0\n",
    "        df_fails['report_type'] = self.report_type\n",
    "\n",
    "        return df_fails, empty_df\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Slices the z3_indicators_master dataframe that agglomerates the data\n",
    "        from all the clients and providers into each table of the star schema\n",
    "        format, for that it creates the id for each table.\n",
    "\n",
    "        After that loads the tables into the z3_results database.\n",
    "        \"\"\"\n",
    "        self._create_z3_indicators_dataframe_ids()\n",
    "        self._create_star_schema_tables()\n",
    "        self.z3_tables_dictionary: Dict[str, DataFrame] = {\n",
    "            'DATES': self.z3_dates_table,\n",
    "            'REPORTS': self.z3_reports_table,\n",
    "            'CLIENTS': self.z3_clients_table,\n",
    "            'PROVIDERS': self.z3_providers_table,\n",
    "            'INDICATORS': self.z3_indicators_table,\n",
    "            'EXECUTIONS': self.z3_executions_table\n",
    "        }\n",
    "        self._perform_load_queries()\n",
    "\n",
    "    def _create_z3_indicators_dataframe_ids(self):\n",
    "        \"\"\"Executes the method to create the ids for each table.\"\"\"\n",
    "        self._create_daily_id()\n",
    "        self._create_client_id()\n",
    "        self._create_report_id()\n",
    "        self._create_provider_id()\n",
    "        self._create_execution_id()\n",
    "\n",
    "    def _create_daily_id(self):\n",
    "        \"\"\"Takes the master data frame, gets the daily id based on its\n",
    "        value.\"\"\"\n",
    "        self.z3_indicators_master['daily_id'] = self.z3_indicators_master['daily'].copy(\n",
    "        )\n",
    "        change_column_datatype(self.z3_indicators_master, 'daily_id', 'str')\n",
    "        self.z3_indicators_master['daily_id'] = self.z3_indicators_master['daily_id'].apply(\n",
    "            lambda x: x.replace('/', ''))\n",
    "        change_column_datatype(self.z3_indicators_master, 'daily_id', 'int')\n",
    "\n",
    "    def _get_client_id(self, client: str) -> int:\n",
    "        \"\"\"Receives the client name and returns its id.\n",
    "\n",
    "        @param client: the client name.\n",
    "        @return client_id: the client id as an integer.\n",
    "        \"\"\"\n",
    "        return self.client_ids[client]\n",
    "\n",
    "    def _create_client_id(self):\n",
    "        \"\"\"Creates the client_id column, by executing the get_client_id into a\n",
    "        copy of the client names column.\"\"\"\n",
    "        self.z3_indicators_master['client_id'] = self.z3_indicators_master['client'].copy(\n",
    "        )\n",
    "        self.z3_indicators_master['client_id'] = self.z3_indicators_master['client_id'].apply(\n",
    "            self._get_client_id)\n",
    "        change_column_datatype(self.z3_indicators_master, 'client_id', 'int')\n",
    "\n",
    "    def _get_provider_id(self, provider: str) -> int:\n",
    "        \"\"\"Receives the provider name and returns its id.\n",
    "\n",
    "        @param provider: provider name.\n",
    "        @return: the id as an integer.\n",
    "        \"\"\"\n",
    "        return self.provider_ids[provider]\n",
    "\n",
    "    def _create_provider_id(self):\n",
    "        \"\"\"Creates the provider_id column.\"\"\"\n",
    "        self.z3_indicators_master['provider_id'] = self.z3_indicators_master['provider'].copy(\n",
    "        )\n",
    "        self.z3_indicators_master['provider_id'] = self.z3_indicators_master['provider_id'].apply(\n",
    "            self._get_provider_id)\n",
    "        change_column_datatype(self.z3_indicators_master, 'provider_id', 'int')\n",
    "\n",
    "    def _get_report_id(self, type_report: str) -> int:\n",
    "        \"\"\"Receives the report type and returns its id.\n",
    "\n",
    "        :param type_report: the report type.\n",
    "        :return report_id: the report id as an integer.\n",
    "        \"\"\"\n",
    "        return self.report_ids[type_report]\n",
    "\n",
    "    def _create_report_id(self):\n",
    "        \"\"\"Executes the _get_report_id method into the report_id column to get\n",
    "        the ids.\"\"\"\n",
    "        self.z3_indicators_master['report_id'] = self.z3_indicators_master['report_type'].copy(\n",
    "        )\n",
    "        self.z3_indicators_master['report_id'] = self.z3_indicators_master['report_id'].apply(\n",
    "            self._get_report_id)\n",
    "        change_column_datatype(self.z3_indicators_master, 'report_id', 'int')\n",
    "\n",
    "    def _create_execution_id(self):\n",
    "        \"\"\"Creates the execution_id column.\"\"\"\n",
    "        self.z3_indicators_master['execution_id'] = self.z3_indicators_master['execution_date'].copy(\n",
    "        )\n",
    "        change_column_datatype(self.z3_indicators_master,\n",
    "                               'execution_id', 'str')\n",
    "        self.z3_indicators_master['execution_id'] = self.z3_indicators_master['execution_id'].apply(\n",
    "            lambda x: x.replace('/', ''))\n",
    "        self.z3_indicators_master['execution_id'] = self.z3_indicators_master['execution_id'].apply(\n",
    "            lambda x: x.replace(':', ''))\n",
    "        self.z3_indicators_master['execution_id'] = self.z3_indicators_master['execution_id'].apply(\n",
    "            lambda x: x.replace('-', ''))\n",
    "\n",
    "        change_column_datatype(self.z3_indicators_master,\n",
    "                               'execution_id', 'int')\n",
    "\n",
    "    def _create_star_schema_tables(self):\n",
    "        \"\"\"Slices the z3_indicators_master dataframe that agglomerates the data\n",
    "        from all the clients and providers into each table of the star schema\n",
    "        format.\"\"\"\n",
    "        self._get_z3_dates_table()\n",
    "        self._get_z3_reports_table()\n",
    "        self._get_z3_clients_table()\n",
    "        self._get_z3_providers_table()\n",
    "        self._get_z3_indicators_table()\n",
    "        self._get_z3_executions_table()\n",
    "\n",
    "    def _get_z3_dates_table(self):\n",
    "        \"\"\"Copies the data from the master file into another dataframe that\n",
    "        will correspond to the dates table, it filters the column to the only\n",
    "        needed according to each table from the star schema defined.\"\"\"\n",
    "        self.z3_dates_table: DataFrame = self.z3_indicators_master.copy()\n",
    "        self.z3_dates_table: DataFrame = self.z3_dates_table[[\n",
    "            'daily_id', 'daily']]\n",
    "        self.z3_dates_table: DataFrame = create_date_yyyy_mm_dd(\n",
    "            self.z3_dates_table)\n",
    "        self.z3_dates_table = self.z3_dates_table.drop_duplicates(\n",
    "            subset='daily_id')\n",
    "\n",
    "    def _get_z3_reports_table(self):\n",
    "        \"\"\"Copies the data from the master file into another dataframe that\n",
    "        will correspond to the reports table, it filters the column to the only\n",
    "        needed according to each table from the star schema defined.\"\"\"\n",
    "        self.z3_reports_table: DataFrame = self.z3_indicators_master.copy()\n",
    "        self.z3_reports_table = self.z3_reports_table[[\n",
    "            'report_id', 'report_type']]\n",
    "        self.z3_reports_table = self.z3_reports_table.drop_duplicates(\n",
    "            subset='report_id')\n",
    "\n",
    "    def _get_z3_clients_table(self):\n",
    "        \"\"\"Copies the data from the master file into another dataframe that\n",
    "        will correspond to the clients table, it filters the column to the only\n",
    "        needed according to each table from the star schema defined.\"\"\"\n",
    "        self.z3_clients_table: DataFrame = self.z3_indicators_master.copy()\n",
    "        self.z3_clients_table = self.z3_clients_table[['client_id', 'client']]\n",
    "        self.z3_clients_table = self.z3_clients_table.drop_duplicates(\n",
    "            subset='client_id')\n",
    "\n",
    "    def _get_z3_providers_table(self):\n",
    "        \"\"\"Copies the data from the master file into another dataframe that\n",
    "        will correspond to the providers table, it filters the column to the\n",
    "        only needed according to each table from the star schema defined.\"\"\"\n",
    "        self.z3_providers_table: DataFrame = self.z3_indicators_master.copy()\n",
    "        self.z3_providers_table = self.z3_providers_table[[\n",
    "            'provider_id', 'provider']]\n",
    "        self.z3_providers_table = self.z3_providers_table.drop_duplicates(\n",
    "            subset='provider_id')\n",
    "\n",
    "    def _get_z3_indicators_table(self):\n",
    "        \"\"\"Copies the data from the master file into another dataframe that\n",
    "        will correspond to the indicators table, it filters the column to the\n",
    "        only needed according to each table from the star schema defined.\"\"\"\n",
    "        self.z3_indicators_table: DataFrame = self.z3_indicators_master.copy()\n",
    "        if 'scrapper_pos_qty' not in list(self.z3_indicators_table.columns):\n",
    "            self.z3_indicators_table['scrapper_pos_qty'] = 0\n",
    "            self.z3_indicators_table['scrapper_pos_sales'] = 0\n",
    "        elif 'scrapper_curr_on_hand_qty' not in list(self.z3_indicators_table.columns):\n",
    "            self.z3_indicators_table['scrapper_curr_on_hand_qty'] = 0\n",
    "\n",
    "        self.z3_indicators_table = self.z3_indicators_table[\n",
    "            ['client_id', 'report_id', 'provider_id', 'execution_id', 'daily_id', 'scrapper_pos_qty',\n",
    "             'scrapper_pos_sales', 'scrapper_curr_on_hand_qty', 'scrapper_rows']]\n",
    "\n",
    "    def _get_z3_executions_table(self):\n",
    "        \"\"\"Copies the data from the master file into another dataframe that\n",
    "        will correspond to the executions table, it filters the column to the\n",
    "        only needed according to each table from the star schema defined.\"\"\"\n",
    "        self.z3_executions_table: DataFrame = self.z3_indicators_master.copy()\n",
    "        if 'scrapper_pos_qty' not in list(self.z3_executions_table.columns):\n",
    "            self.z3_executions_table['scrapper_pos_qty'] = 0\n",
    "            self.z3_executions_table['scrapper_pos_sales'] = 0\n",
    "        elif 'scrapper_curr_on_hand_qty' not in list(self.z3_executions_table.columns):\n",
    "            self.z3_executions_table['scrapper_curr_on_hand_qty'] = 0\n",
    "\n",
    "        self.z3_executions_table = self.z3_executions_table[[\n",
    "            'execution_id', 'execution_date']]\n",
    "        self.z3_executions_table: DataFrame = create_date_yyyy_mm_dd_hh_mins(\n",
    "            self.z3_executions_table, 'execution_date')\n",
    "\n",
    "    def _perform_load_queries(self) -> DataFrame:\n",
    "        \"\"\"Creates the tables and inserts the data into the z3_results\n",
    "        database.\"\"\"\n",
    "        db_name: str = \"z3_results\"\n",
    "        connection: str = f\"host={self.z3_results_host_db} dbname={db_name} user={self.z3_results_user_db} password={self.z3_results_password_db}\"\n",
    "        conn = psycopg2.connect(connection)\n",
    "        try:\n",
    "            conn.set_session(autocommit=True, readonly=False)\n",
    "            cur = conn.cursor()\n",
    "\n",
    "            self._create_tables(cur)\n",
    "            self._insert_tables(cur)\n",
    "\n",
    "        except psycopg2.Error as e:\n",
    "            logging.info(e)\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_tables(cur):\n",
    "        \"\"\"Runs the creating tables extract_and_quality.\n",
    "\n",
    "        @cur: database cursor\n",
    "        @conn: database connection\n",
    "        \"\"\"\n",
    "        for query in create_table_queries:\n",
    "            cur.execute(query)\n",
    "\n",
    "    def _insert_tables(self, cur):\n",
    "        \"\"\"Distributes the information into each table from the star schema.\n",
    "\n",
    "        @cur: database cursor\n",
    "        @conn: database connection\n",
    "        \"\"\"\n",
    "        for key in list(insert_table_queries_dict.keys()):\n",
    "            query: str = insert_table_queries_dict[key]\n",
    "            dataframe: DataFrame = self.z3_tables_dictionary[key]\n",
    "            for _, row in dataframe.iterrows():\n",
    "                cur.execute(query, list(row))\n",
    "\n",
    "    def data_quality_checks(self):\n",
    "        \"\"\"Tests if the data processed corresponds to the data loaded into the\n",
    "        database.\"\"\"\n",
    "        database_result: DataFrame = self._perform_data_quality_query()\n",
    "        database_pos_qty: float = database_result['database_pos_qty'].sum()\n",
    "        database_pos_sales: float = database_result['database_pos_sales'].sum()\n",
    "        database_curr_on_hand_qty: float = database_result['database_curr_on_hand_qty'].sum(\n",
    "        )\n",
    "        database_rows: float = database_result['database_rows'].sum()\n",
    "        database_dailys: List[str] = database_result['daily'].unique()\n",
    "\n",
    "        scrapper_pos_qty: float = self.z3_indicators_table['scrapper_pos_qty'].sum(\n",
    "        )\n",
    "        scrapper_pos_sales: float = self.z3_indicators_table['scrapper_pos_sales'].sum(\n",
    "        )\n",
    "        scrapper_rows: float = self.z3_indicators_master['scrapper_rows'].sum()\n",
    "        scrapper_curr_on_hand_qty: float = self.z3_indicators_table['scrapper_curr_on_hand_qty'].sum(\n",
    "        )\n",
    "        scrapper_dailys: List[str] = self.z3_dates_table['daily'].unique()\n",
    "\n",
    "        assert database_pos_qty == pytest.approx(scrapper_pos_qty, 0.2)\n",
    "        assert database_pos_sales == pytest.approx(scrapper_pos_sales, 0.2)\n",
    "        assert database_curr_on_hand_qty == pytest.approx(\n",
    "            scrapper_curr_on_hand_qty, 0.2)\n",
    "        assert database_rows == pytest.approx(scrapper_rows, 0.2)\n",
    "        assert all(record in scrapper_dailys for record in database_dailys)\n",
    "\n",
    "    def _perform_data_quality_query(self) -> DataFrame:\n",
    "        \"\"\"Queries the z3_results database, and takes the result into a\n",
    "        dataframe.\"\"\"\n",
    "        db_name: str = \"z3_results\"\n",
    "        connection: str = f\"host={self.z3_results_host_db} dbname={db_name} user={self.z3_results_user_db} \" \\\n",
    "                          f\"password={self.z3_results_password_db}\"\n",
    "        df_quality: DataFrame = pd.DataFrame()\n",
    "        conn = psycopg2.connect(connection)\n",
    "        try:\n",
    "            conn.set_session(autocommit=True, readonly=True)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(quality_checks)\n",
    "\n",
    "            df_quality: DataFrame = DataFrame(\n",
    "                cur.fetchall(),\n",
    "                columns=columns_quality_checks,\n",
    "            )\n",
    "        except psycopg2.Error as e:\n",
    "            logging.info(e)\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "        return df_quality\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_tables():\n",
    "        \"\"\"Drops all the tables in the z3_results database.\"\"\"\n",
    "        load_dotenv(find_dotenv())\n",
    "        z3_results_password_db: str = os.getenv(\"RESULTS_PASSWORD\")\n",
    "        z3_results_user_db: str = os.getenv(\"RESULTS_USER_DB\")\n",
    "        z3_results_host_db: str = os.getenv(\"RESULTS_HOST_DB\")\n",
    "        db_name: str = \"z3_results\"\n",
    "        connection: str = f\"host={z3_results_host_db} dbname={db_name} \" \\\n",
    "                          f\"user={z3_results_user_db} password={z3_results_password_db}\"\n",
    "        conn = psycopg2.connect(connection)\n",
    "        try:\n",
    "            conn.set_session(autocommit=True, readonly=False)\n",
    "            cur = conn.cursor()\n",
    "            for query in drop_table_queries:\n",
    "                cur.execute(query)\n",
    "        except psycopg2.Error as e:\n",
    "            logging.info(e)\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def main(self):\n",
    "        \"\"\"Directs the execution order of each method.\"\"\"\n",
    "        self.extract_and_transform_each_provider_and_client()\n",
    "        self.load()\n",
    "        self.data_quality_checks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b3488-ca0e-44d1-9cc7-0d9bba2606da",
   "metadata": {},
   "source": [
    "## Step 5: Complete project write up\n",
    "#### Whats' the goal?\n",
    "The goal is to make sure data makes sense once it is into the scrappers database, since that data is going into the clients BI side.\n",
    "\n",
    "### How would Spark or Airflow be incorporated?\n",
    "The project is kind of thinked to be taken into an airflow dag, each of the steps is a clearly defined action that can be re-coded into python operators, spark will be implemented in case the data amount grows exponentially, at this point after summarizing the results the cost of processing is low, so that's the only way that implementing spark would be worth doing.\n",
    "\n",
    "\n",
    "#### Choice of technologies\n",
    "* Database: Postgresql as the open source, user friendly technology and big data capabilites once the data amount grows.\n",
    "* Data wrangling: Pandas, one of the best libraries to manipulate and analyze dataframes, I also have plenty of experience with pandas.\n",
    "* User Interface: Jupyter lab: As a easy to use python interface that let's you run the code with a simple click.\n",
    "\n",
    "#### Data updates\n",
    "The data must be updating everyday at 9:30 am, in order to spot useless data and have time to regenerate the reports before the client sees it on the Bussiness Intelligence report.\n",
    "\n",
    "#### Scenarios\n",
    "* Data was increased to 100x: In this case the option can be to implement Spark into the project to run the etl on parallel with EMR instances. Other solution would be to add more processing units to each database in order to accomplish the extract query, probably the other step would be to separate each client into its z3 database, since I am putting all the data togheter into one database. \n",
    "* 7am update: Get the project into an airflow DAG, so nobody will have to execute the jupyter lab manually.\n",
    "* If the database needed to be accessed by 100+ people: Implement a distributed database in order to have high availability.\n",
    "Another solution is to increase the database hardware, another way to solve this depending on the needs is to implement views to 'pre-load' the most complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd9959-7895-408e-b1be-987e3ce2e30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
